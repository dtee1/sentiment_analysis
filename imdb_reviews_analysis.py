# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mbip4p7xYU_K3C8NvsSzRhO5tTapa9mY
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import os
import pickle

import numpy as np
import tensorflow as tf
from keras.layers import Dense, Embedding, LSTM
from keras.models import Sequential
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import SimpleRNN, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import tarfile
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from collections import Counter
import itertools
from prettytable import PrettyTable
from tensorflow.keras.optimizers import Adam

class IMDbReviewsDatasetPreparer:
  """
  Class for preparing IMDB reviews dataset for model training and testing
  IMDB movie reviews can be downloaded from: https://ai.stanford.edu/~amaas/data/sentiment/
  GloVe pre-trained embedding vector can be downloaded from: https://nlp.stanford.edu/projects/glove/

  Note: As of the time of time of this work, the move reviews contain 25,000 train and test data each

  Methods:
  - generate_data: Generates or loads preprocessed dataset
  - lemmatize:  Lemmatizes tokens using NLTK's WordNetLemmatizer
  - handle_negation: Handles negations in a list of tokens (ex. 'not good' to 'not_good')
  - read_data: Reads movie reviews and labels from the IMDb data
  - load_glove_embeddings: Loads pre-trained GloVe word embeddings

  Attributes:
    stop_words: Set of English stop words used for text preprocessing

  """
  def __init__(self, directory, aclImdb, glove):
    print(aclImdb)
    self.stop_words = set(stopwords.words('english'))
    self.train_data, self.test_data = self.generate_data(directory, aclImdb)
    self.embeddings = self.load_glove_embeddings(directory, glove)

  def generate_data(self, directory, aclImdb):

    train_data_file = os.path.join(directory, 'train_data.pkl')
    test_data_file = os.path.join(directory, 'test_data.pkl')

    # Check if preprocessed data already exists in pickle files
    if os.path.exists(train_data_file) and os.path.exists(test_data_file):
      print("Loading train and test dataset from pickle files...")
      with open(train_data_file, 'rb') as file:
        train_data = pickle.load(file)
      with open(test_data_file, 'rb') as file:
        test_data = pickle.load(file)
      return train_data, test_data

    # Prepare dataset if pickle files do not exist
    destination_folder = os.path.join(directory, aclImdb)
    tar_path = os.path.join(directory, 'aclImdb_v1.tar.gz')
    if os.path.exists(destination_folder):
      print("aclImdb folder folder found... Preparing dataset")
    elif os.path.exists(tar_path):
      print("aclImdb_v1.tar.gz found, extracting data...")
      try:
        with tarfile.open(tar_path, 'r:gz') as tar:
          tar.extractall(path=destination_folder)
          print("Extraction successful...")
      except Exception as e:
        print(f"Error extracting the file: {e}")
    else:
      print("Neither aclImdb data nor aclImdb_v1.tar.gz was found, exiting...")
      print("Please download data from: https://ai.stanford.edu/~amaas/data/sentiment/")
      return None, None

    train_reviews, train_labels = self.read_data(destination_folder, 'train')
    test_reviews, test_labels = self.read_data(destination_folder, 'test')
    print("Data reading completed.")

    # Save data in pickle format
    print("Saving data for future use...")
    with open(train_data_file, 'wb') as file:
      pickle.dump((train_reviews, train_labels), file)
    with open(test_data_file, 'wb') as file:
      pickle.dump((test_reviews, test_labels), file)
    return (train_reviews, train_labels), (test_reviews, test_labels)

  def lemmatize(self, tokens):
    """
    Lemmatizes tokens using NLTK's WordNetLemmatizer.
    Parameters:
    - tokens: List of tokens to lemmatize
    Returns:
    - list: Lemmatized list of tokens
    """
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = []
    try:
        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
    except Exception as e:
        print(f"Error occured in lemmatization: {e}")
    return lemmatized_tokens

  def handle_negation(self, tokens):
    """
    Handles negations in a list of tokens (e.g., 'not good' to 'not_good')
    Parameters:
    - tokens (list): List of tokens to handle negations
    Returns:
    - list: Tokens with negations handled
    """
    try:
      for i in range(len(tokens)):
        if tokens[i] == 'not' and i < len(tokens) - 1:
          tokens[i + 1] = 'not_' + tokens[i + 1]
    except Exception as e:
      print(f"Error occured in negation handling: {e}")
    return tokens

  def read_data(self, directory, data_type):
    """
    Reads movie reviews, ratings, and labels from the IMDb dataset
    Parameters:
    - directory (str): Directory containing the IMDb dataset
    - data_type (str): Type of data ('train' or 'test')
    Returns:
      tuple: A tuple containing a list of dictionaries representing reviews and a list of corresponding labels
    """
    reviews = []
    labels = []
    aclImdb = os.path.join(directory, f'aclImdb/{data_type}')

    # Check if the specified data type directory exists
    if os.path.exists(aclImdb):
      # Read files in the 'pos' and 'neg' folders
      for label in ['pos', 'neg']:
        path = os.path.join(aclImdb, label)
        for filename in os.listdir(path):
          print(f"Reading file: {filename}")
          with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:
            # Extract movie id and rating from the filename
            unique_id, rating = filename[:-4].split('_')
            review = file.read()

            # Tokenize, remove stop words, lemmatize, and handle negations in each review
            tokens = word_tokenize(review)
            tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in self.stop_words]
            tokens = self.lemmatize(tokens)
            tokens = self.handle_negation(tokens)

            reviews.append({'id': int(unique_id), 'text': tokens, 'rating': int(rating)})
            labels.append(1 if label == 'pos' else 0)
      print(f"{data_type.capitalize()} data reading completed.")
    return reviews, labels

  def load_glove_embeddings(self, directory, glove):
    """
    Loads pre-trained GloVe word embeddings
    Parameters:
    - directory (str): Directory containing GloVe embeddings file
    Returns:
    - dict: Dictionary conatining words to their corresponding embedding vectors
    """
    embeddings = {}
    embeddings_file_pkl = os.path.join(directory, 'glove_embeddings.pkl')

    # Check if preprocessed embedding pickle file exists
    if os.path.exists(embeddings_file_pkl):
      try:
        with open(embeddings_file_pkl, 'rb') as file:
          embeddings = pickle.load(file)
        return embeddings
      except (pickle.UnpicklingError, EOFError) as e:
        print(f"Error loading GloVe embeddings from pickle file: {e}")
        print("Attempting to load from the original GloVe text file...")

    # Load from original GloVe text file
    embeddings_file = os.path.join(directory, glove)

    if os.path.exists(embeddings_file):
      with open(embeddings_file, encoding='utf-8') as f:
          for line in f:
              values = line.split()
              word = values[0]
              coefs = np.asarray(values[1:], dtype='float32')
              embeddings[word] = coefs
    else:
      print("Neither glove_embeddings.pkl nor glove.42B.300d.txt found")
      print("Please download data from: https://nlp.stanford.edu/projects/glove/")

    # Saving word embeddings as pickle file
    with open(embeddings_file_pkl, 'wb') as file:
        pickle.dump(embeddings, file)
    return embeddings

  def exploratory_data_analysis(self):
    """
    Conducts exploratory data analysis on the dataset.
    """
    if self.train_data is None or self.test_data is None:
      print("Dataset not loaded")
    return

    train_reviews, train_labels = self.train_data
    test_reviews, test_labels = self.test_data

    # Visualize the distribution of labels in the training and testing datasets
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    sns.countplot(train_labels)
    plt.title('Distribution of Labels in Training Data')

    plt.subplot(1, 2, 2)
    sns.countplot(test_labels)
    plt.title('Distribution of Labels in Testing Data')

    plt.show()

    # Display some sample reviews
    print("\nSample Reviews:")
    for i in range(5):
        print(f"\nReview #{i + 1}:")
        print(train_reviews[i]['text'])
        print(f"Label: {'Positive' if train_labels[i] == 1 else 'Negative'}")

directory_path = '/content/drive/MyDrive/masters_program/Fall_2023/'
aclImdb = 'aclImdb'
glove = 'glove.42B.300d.txt'
sentiment_analysis = IMDbReviewsDatasetPreparer(directory_path, aclImdb, glove)
train_data  = sentiment_analysis.train_data
test_data = sentiment_analysis.test_data

class ModelTrainer:
  """
  Class to build, train, and evaluate a vanilla recurrent neural network and l
  Parameters:
  - train_data: Tuple list containing movie reviews and labels for training
  - test_data:  Tuple list containing movie reviews and labels for testing
  - glove_embeddings: Dictionary containing word embedding vectors
  - max_words: Maximum unique words to consider
  - emdedding_dim: Dimensionality of word embeddings
  - input_length: Lnegth of input sequences

  Methods:
  - create_word_index_mapping: Word-to-index and index-to-word mappings
  - create_embedding_matrix: Embedding matrix from pre-trained word embeddings
  - build_rnn_model: Build and compile an RNN model
  - build_lstm_model: Build and compile an LSTM model
  - train_model: Train both RNN and LSTM models
  - evaluate_model: Evaluate and print the test accuracy of both RNN and LSTM models.
  """
  def __init__(self, train_data, test_data, glove_embeddings, max_words=5000, embedding_dim=300):
    self.max_words = max_words
    self.embedding_dim = embedding_dim
    self.train_data = train_data
    self.test_data = test_data
    self.train_reviews, _ = train_data
    self.glove_embeddings = glove_embeddings
    self.word_to_index, self.index_to_word = self.create_word_index_mapping()
    self.embedding_matrix = self.create_embedding_matrix()

  def create_word_index_mapping(self):
    all_words = [word for review in self.train_reviews for word in review['text']]
    unique_words = list(set(all_words))
    num_words = min(self.max_words, len(unique_words))
    word_frequencies = Counter(all_words)
    top_words = [word for word, _ in word_frequencies.most_common(self.max_words)]
    word_to_index = {word: index + 1 for index, word in enumerate(top_words)}
    index_to_word = {index: word for word, index in word_to_index.items()}
    return word_to_index, index_to_word

  def create_embedding_matrix(self):
    embedding_matrix = np.zeros((len(self.word_to_index) + 1, self.embedding_dim))
    for word, i in self.word_to_index.items():
      if i <= self.max_words:
        embedding_vector = self.glove_embeddings.get(word)
        if embedding_vector is not None:
          embedding_matrix[i] = embedding_vector
    return embedding_matrix

  def build_rnn_model(self,dropout=0.2,state_dimension=20):
    model = Sequential()
    model.add(Embedding(input_dim=len(self.word_to_index) + 1, output_dim=self.embedding_dim,
                        input_length=state_dimension, weights=[self.embedding_matrix], trainable=False))
    model.add(SimpleRNN(units=128, activation='relu', dropout=dropout))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

  def build_lstm_model(self,dropout=0.2,state_dimension=20):
    model = Sequential()
    model.add(Embedding(input_dim=len(self.word_to_index) + 1, output_dim=self.embedding_dim,
                        input_length=state_dimension, weights=[self.embedding_matrix], trainable=False))
    model.add(LSTM(units=128, activation='relu', dropout=dropout))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

  def train_model(self, rnn_model, lstm_model, epochs=10, batch_size=32, maxlen=20):
    _, train_labels = self.train_data
    test_reviews, test_labels = self.test_data
    train_sequences = [[self.word_to_index.get(word, 0) for word in review['text']] for review in self.train_reviews]
    train_sequences, val_sequences, train_labels, val_labels = train_test_split(train_sequences, train_labels, test_size=0.2, random_state=42)
    train_sequences = pad_sequences(train_sequences, maxlen= maxlen)
    val_sequences = pad_sequences(val_sequences, maxlen=maxlen)
    train_labels = np.array(train_labels)
    val_labels = np.array(val_labels)
    rnn_model.fit(train_sequences, train_labels, epochs=epochs, batch_size=64, validation_data=(val_sequences, val_labels), verbose=0)
    _, rnn_accuracy = rnn_model.evaluate(val_sequences, val_labels)
    print(f"RNN Validation Accuracy: {rnn_accuracy * 100:.2f}%\n")
    lstm_model.fit(train_sequences, train_labels, epochs=epochs, batch_size=64, validation_data=(val_sequences, val_labels), verbose=0)
    _, lstm_accuracy = lstm_model.evaluate(val_sequences, val_labels)
    print(f"LSTM Validation Accuracy: {lstm_accuracy * 100:.2f}%\n")

  def evaluate_model(self, rnn_model, lstm_model, maxlen=20):
    test_reviews, test_labels = self.test_data
    test_sequences = [[self.word_to_index.get(word, 0) for word in review['text']] for review in test_reviews]
    test_sequences = pad_sequences(test_sequences, maxlen=maxlen)
    test_labels =  np.array(test_labels)
    _, rnn_accuracy = rnn_model.evaluate(test_sequences, test_labels)
    _, lstm_accuracy = lstm_model.evaluate(test_sequences, test_labels)
    return rnn_accuracy, lstm_accuracy

"""# Hyper Parameter Tuning


"""

state_dimensions = [20, 50, 100, 200, 500]
batch_sizes = [32, 64]
dropouts = [0.2, 0.4, 0.6]
table = PrettyTable()
table.field_names = ["Maxlen", "Batch Size", "Dropout", "RNN Accuracy", "LSTM Accuracy"]
model_trainer = ModelTrainer(sentiment_analysis.train_data, sentiment_analysis.test_data, sentiment_analysis.embeddings, 5000, 300)

for state_dimension, batch_size, dropout in itertools.product(state_dimensions, batch_sizes, dropouts):
  print(f"\nTraining and Evaluating for maxlen={state_dimension}, batch_size={batch_size}, dropout={dropout}:\n")
  rnn_model = model_trainer.build_rnn_model(dropout=dropout, state_dimension=state_dimension)
  lstm_model = model_trainer.build_lstm_model(dropout=dropout, state_dimension=state_dimension)
  model_trainer.train_model(rnn_model,lstm_model, epochs=3, batch_size=batch_size, maxlen=state_dimension)
  rnn_accuracy, lstm_accuracy = model_trainer.evaluate_model(rnn_model,lstm_model,maxlen=state_dimension)
  table.add_row([state_dimension, batch_size, dropout, f"{rnn_accuracy * 100:.2f}%", f"{lstm_accuracy * 100:.2f}%"])

"""# Further Hyper Parameter Tuning"""

state_dimensions = [20, 50, 100, 200, 500]
batch_sizes = [32]
dropouts = [0.2]
table = PrettyTable()
table.field_names = ["Maxlen", "Batch Size", "Dropout", "RNN Accuracy", "LSTM Accuracy"]
model_trainer = ModelTrainer(sentiment_analysis.train_data, sentiment_analysis.test_data, sentiment_analysis.embeddings, 5000, 300)

for state_dimension, batch_size, dropout in itertools.product(state_dimensions, batch_sizes, dropouts):
  print(f"\nTraining and Evaluating for maxlen={state_dimension}, batch_size={batch_size}, dropout={dropout}:\n")
  rnn_model = model_trainer.build_rnn_model(dropout=dropout, state_dimension=state_dimension)
  lstm_model = model_trainer.build_lstm_model(dropout=dropout, state_dimension=state_dimension)
  model_trainer.train_model(rnn_model,lstm_model, epochs=3, batch_size=batch_size, maxlen=state_dimension)
  rnn_accuracy, lstm_accuracy = model_trainer.evaluate_model(rnn_model,lstm_model,maxlen=state_dimension)
  table.add_row([state_dimension, batch_size, dropout, f"{rnn_accuracy * 100:.2f}%", f"{lstm_accuracy * 100:.2f}%"])

table.sortby = "RNN Accuracy"
table.reversesort = True
print("Combination with the highest RNN Accuracy:")
print(table)

table.sortby = "LSTM Accuracy"
table.reversesort = True
print("\nCombination with the highest LSTM Accuracy:")
print(table)

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))